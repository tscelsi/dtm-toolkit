{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Labelling Examples\n",
    "\n",
    "For more involved examples, see ./dtm_analysis_examples.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtm_toolkit.auto_labelling import AutoLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create an example thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesaurus = {\n",
    "    \"soccer\": \"sport\",\n",
    "    \"basketball\": \"sport\",\n",
    "    \"painting\": \"art\",\n",
    "    \"musician\": \"art\",\n",
    "    \"tennis\": \"sport\",\n",
    "    \"tennis player\": \"sport\",\n",
    "    \"ronaldo\": \"sport\",\n",
    "    \"lebron james\": \"sport\",\n",
    "    \"james taylor\": \"art\",\n",
    "    \"strings\": \"art\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the auto labelling class\n",
    "autolabel = AutoLabel(thesaurus, phrase_col=None, label_col=None, spacy_lang=\"en_core_web_sm\", preprocess=False, n_process=None, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('sport', 0.06)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Here we simulate a particular topic that pertains to sports, we hope it gets labelled correctly.\n",
    "\"\"\"\n",
    "topic = [(0.6, 'basketball'), (0.04, 'layup'), (0.02, 'referee'), (0.01, 'court'), (0.01, 'tennis'), (0.3, 'strings'), (0.02, 'run')]\n",
    "labels = autolabel.get_topic_labels(topics=[topic], score_type=\"tfidf\")\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'art': 0.04158883083359671, 'sport': 0.05545177444479562})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"We see that the score for 'sport' label is marginally higher than 'art'.\"\"\"\n",
    "autolabel.get_topic_labels(topics=[topic], raw=True, score_type=\"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising gloVe embeddings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Counter({'art': 0.58206457, 'sport': 0.8463534712791443})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Let's try this time with the embedding strategy which doesn't rely on exact matches, but instead\n",
    "on the contextual embeddings of each term in the top topic terms. We see that 'sport' receives a \n",
    "much higher score than 'art' in this case.\n",
    "\"\"\"\n",
    "labels = autolabel.get_topic_labels(topics=[topic], raw=True)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a8aab45800d903aedf66933c8084899e59800547e5a841112d5e4b57b4713698"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
